{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "from bson import ObjectId\n",
    "\n",
    "# Create a MongoClient object and specify the MongoDB connection URL\n",
    "url = \"mongodb://localhost:27017/\"\n",
    "client1 = MongoClient(url)\n",
    "\n",
    "# Access a specific database\n",
    "db1 = client1[\"youtube_twitter_db1\"]\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client1.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter collection creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV and collection creation\n",
    "tw_collection = db1.tw_collection\n",
    "i = 0\n",
    "\n",
    "with open('./df_youtube.csv', 'r') as file:\n",
    "    csv_data = csv.DictReader(file)\n",
    "    for row in csv_data:\n",
    "        if row[\"urls_list\"] is not None:\n",
    "\n",
    "            try:\n",
    "                urls_list = json.loads(row[\"urls_list\"].replace(\"'\", '\"'))\n",
    "                expanded_urls = [item.get(\"expanded_url\") for item in urls_list]\n",
    "                \n",
    "                #new column creation containing the list of expandend urls\n",
    "                row[\"expanded_url\"] = expanded_urls\n",
    "                tw_collection.insert_one(row)\n",
    "\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                print(f\"Error line {i} Url_list {row['urls_list']}\")\n",
    "        \n",
    "        \n",
    "        if (i % 10000) == 0:\n",
    "            print(i)\n",
    "        i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube collection creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV and collection creation\n",
    "\n",
    "yt_collection = db1.yt_collection\n",
    "with open('/Users/raffaelerusso/Documents/GitHub/Youtube-Video-Classification-on-Twitter/VideoClassification/y_train.csv', 'r') as file:\n",
    "    csv_data = csv.DictReader(file)\n",
    "    for row in csv_data:\n",
    "       yt_collection.insert_one(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associate each yt video to a list of the referencing tweet ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/18\n",
      "Processed batch 2/18\n",
      "Processed batch 3/18\n",
      "Processed batch 4/18\n",
      "Processed batch 5/18\n",
      "Processed batch 6/18\n",
      "Processed batch 7/18\n",
      "Processed batch 8/18\n",
      "Processed batch 9/18\n",
      "Processed batch 10/18\n",
      "Processed batch 11/18\n",
      "Processed batch 12/18\n",
      "Processed batch 13/18\n",
      "Processed batch 14/18\n",
      "Processed batch 15/18\n",
      "Processed batch 16/18\n",
      "Processed batch 17/18\n",
      "Processed batch 18/18\n"
     ]
    }
   ],
   "source": [
    "# Collect all id values from yt_collection\n",
    "id_values = [doc['id'] for doc in yt_collection.find({}, {'id': 1})]\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Split id_values into batches\n",
    "id_batches = [id_values[i:i+batch_size] for i in range(0, len(id_values), batch_size)]\n",
    "\n",
    "# Create a dictionary to store the matching tweet_ids for each id\n",
    "id_tweet_ids = {id_val: set() for id_val in id_values}\n",
    "\n",
    "# Iterate over each batch and collect the matching tweet_ids\n",
    "for batch_index, id_batch in enumerate(id_batches):\n",
    "    pattern = \"|\".join(map(re.escape, id_batch))\n",
    "\n",
    "    # Find documents in tw_collection where expanded_url contains any of the id values in the batch\n",
    "    results = tw_collection.find(\n",
    "        {\"expanded_url\": {\"$elemMatch\": {\"$regex\": pattern}}},\n",
    "        {\"tweetid\": 1, \"expanded_url\": 1}\n",
    "    )\n",
    "   \n",
    "    # Update id_tweet_ids dictionary with the matching tweet_ids\n",
    "    for doc in results:\n",
    "        tweet_id = doc[\"tweetid\"]\n",
    "        expanded_url_list = doc[\"expanded_url\"]\n",
    "        for expanded_url in expanded_url_list:\n",
    "            for id_val in id_batch:\n",
    "                if id_val in expanded_url:\n",
    "                    id_tweet_ids[id_val].add(tweet_id)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Processed batch {batch_index+1}/{len(id_batches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching tweet_ids updated in yt_collection.\n"
     ]
    }
   ],
   "source": [
    "# Update yt_collection with the tweet_ids for each id\n",
    "for id_val, tweet_ids in id_tweet_ids.items():\n",
    "    yt_collection.update_one(\n",
    "        {\"id\": id_val},\n",
    "        {\"$set\": {\"tweet_ids\": list(tweet_ids)}}\n",
    "    )\n",
    "\n",
    "# Print completion message\n",
    "print(\"Matching tweet_ids updated in yt_collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_tweet_ids saved to file.\n"
     ]
    }
   ],
   "source": [
    "id_tweet_ids_list = {k: list(v) for k, v in id_tweet_ids.items()}\n",
    "\n",
    "# Save id_tweet_ids to a file\n",
    "with open(\"id_tweet_ids.json\", \"w\") as file:\n",
    "    json.dump(id_tweet_ids_list, file)\n",
    "\n",
    "print(\"id_tweet_ids saved to file.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the new y_train CSV with the updated list of tweet ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved to file.\n"
     ]
    }
   ],
   "source": [
    "path_labels = \"./y_train.csv\"\n",
    "youtube_df = pd.read_csv(path_labels)\n",
    "\n",
    "# Creation of the new dataframe\n",
    "df = youtube_df.assign(tweet_ids=lambda x: x[\"id\"].map(id_tweet_ids_list))\n",
    "\n",
    "# Convert \"not moderated\" to 0 and \"moderated\" to 1\n",
    "df[\"moderationStatus\"] = df[\"moderationStatus\"].replace({\"not moderated\": 0, \"moderated\": 1})\n",
    "\n",
    "# Dropping column \"lista_url\"\n",
    "df = df.drop(\"lista_url\",axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"updated_merged.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"Updated dataset saved to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>moderationStatus</th>\n",
       "      <th>tweet_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2bFLr70bNzA</td>\n",
       "      <td>0</td>\n",
       "      <td>['1319671748170797062', '1319504474956845056', '1319431259194609664', '1319537482082492416', '1319452511527460864', '1319664735915159553', '1319807337243181062']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-gWzKGaj6Ss</td>\n",
       "      <td>1</td>\n",
       "      <td>['1322068734652125184', '1321990250500104192', '1321989314784436225', '1322170572856528896', '1322013423954239489', '1322129221469249536', '1322156265087393793', '1322127027181334528', '1322055781940944897', '1322056591294816256', '1321981266367950849', '1322063743258398723', '1321972961310629889']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BHnJp0oyOxs</td>\n",
       "      <td>0</td>\n",
       "      <td>['1320793071206977537', '1320824145827950592', '1319972970299838464', '1320772899691745280', '1322224750631018497', '1320770467632852993', '1320804289808371716', '1320755036180213761', '1320848677938909185', '1318469485985148928', '1317074899287105540', '1320806383818809349', '1320960304872787974', '1320795016613273603', '1315132275034652673', '1320747645850292226', '1320789711158272000', '1315131864093515776']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q77rJcVbDAI</td>\n",
       "      <td>0</td>\n",
       "      <td>['1318484961788387328', '1318473684793831425', '1318437517016788992']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RGhwHbp66P4</td>\n",
       "      <td>0</td>\n",
       "      <td>['1321324715949002752', '1318460466595758080', '1317760528555692040', '1313146199453642755', '1311457744550064130']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  moderationStatus  \\\n",
       "0  2bFLr70bNzA                 0   \n",
       "1  -gWzKGaj6Ss                 1   \n",
       "2  BHnJp0oyOxs                 0   \n",
       "3  q77rJcVbDAI                 0   \n",
       "4  RGhwHbp66P4                 0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                        tweet_ids  \n",
       "0                                                                                                                                                                                                                                                               ['1319671748170797062', '1319504474956845056', '1319431259194609664', '1319537482082492416', '1319452511527460864', '1319664735915159553', '1319807337243181062']  \n",
       "1                                                                                                                     ['1322068734652125184', '1321990250500104192', '1321989314784436225', '1322170572856528896', '1322013423954239489', '1322129221469249536', '1322156265087393793', '1322127027181334528', '1322055781940944897', '1322056591294816256', '1321981266367950849', '1322063743258398723', '1321972961310629889']  \n",
       "2  ['1320793071206977537', '1320824145827950592', '1319972970299838464', '1320772899691745280', '1322224750631018497', '1320770467632852993', '1320804289808371716', '1320755036180213761', '1320848677938909185', '1318469485985148928', '1317074899287105540', '1320806383818809349', '1320960304872787974', '1320795016613273603', '1315132275034652673', '1320747645850292226', '1320789711158272000', '1315131864093515776']  \n",
       "3                                                                                                                                                                                                                                                                                                                                                           ['1318484961788387328', '1318473684793831425', '1318437517016788992']  \n",
       "4                                                                                                                                                                                                                                                                                                             ['1321324715949002752', '1318460466595758080', '1317760528555692040', '1313146199453642755', '1311457744550064130']  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the csv as a collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection data saved as JSON.\n"
     ]
    }
   ],
   "source": [
    "# Custom JSON encoder class to handle ObjectId serialization\n",
    "class JSONEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, ObjectId):\n",
    "            return str(o)\n",
    "        return super().default(o)\n",
    "\n",
    "# Retrieve all documents from the collection\n",
    "all_documents = list(yt_collection.find())\n",
    "\n",
    "# Specify the path and filename for the output JSON file\n",
    "output_file = \"collection_data.json\"\n",
    "\n",
    "# Save the collection data to the JSON file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(all_documents, file, cls=JSONEncoder)\n",
    "\n",
    "print(\"Collection data saved as JSON.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtube_twitter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
